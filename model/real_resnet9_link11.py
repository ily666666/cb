import torch
import torch.nn as nn

class BasicBlock(nn.Module):
    expansion = 1
    def __init__(self, in_channels, out_channels, stride=1, downsample=None):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.downsample = downsample

    def forward(self, x):
        identity = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        if self.downsample is not None:
            identity = self.downsample(x)
        out += identity
        out = self.relu(out)
        return out


  # æ³¨æ„ï¼šéœ€è¦å°†BasicBlockæ›¿æ¢ä¸ºBottleneckå—ï¼ˆä¸æ•™å¸ˆæ¨¡å‹ä¸€è‡´ï¼‰
class Bottleneck(nn.Module):
    expansion = 4  # æ‰©å±•å› å­ï¼Œè¾“å‡ºé€šé“=out_channelsÃ—4
    def __init__(self, in_channels, out_channels, stride=1, downsample=None):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, 
                               stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, 
                               kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample

    def forward(self, x):
        identity = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)
        out = self.conv3(out)
        out = self.bn3(out)
        if self.downsample is not None:
            identity = self.downsample(x)
        out += identity
        out = self.relu(out)
        return out

class ResNet9Real(nn.Module):
    def __init__(self, num_classes=7):
        super(ResNet9Real, self).__init__()
        self.in_channels = 64  # æ”¹ä¸º64ï¼ŒåŒ¹é…æ•™å¸ˆæ¨¡å‹åˆå§‹é€šé“
        self.conv1 = nn.Conv2d(2, 64, kernel_size=7, stride=2, padding=3, bias=False)  # æ¨¡ä»¿æ•™å¸ˆåˆå§‹å±‚
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        
        # ResNet9: ä½¿ç”¨ Bottleneck (expansion=4) ä½†ä¿æŒæ¯ stage 1 ä¸ª block (æµ…å±‚)
        # é€šé“æ•°å¯¹é½ ResNet50: [256, 512, 1024]
        
        # layer1ï¼šArg out=64 -> å®é™…è¾“å‡º 64*4 = 256
        self.layer1 = self._make_layer(in_channels=64, out_channels=64, blocks=1, stride=1)
        # layer2ï¼šArg out=128 -> å®é™…è¾“å‡º 128*4 = 512
        self.layer2 = self._make_layer(in_channels=256, out_channels=128, blocks=1, stride=2)
        # layer3ï¼šArg out=256 -> å®é™…è¾“å‡º 256*4 = 1024
        self.layer3 = self._make_layer(in_channels=512, out_channels=256, blocks=1, stride=2)
        
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.intermediate_fc = nn.Linear(1024, 1000)  # 1024â†’1000
        self.fc = nn.Linear(1000, num_classes)  # 1000â†’21

        # =================== æ–°å¢ï¼šé¢‘åŸŸé‡å»ºå¤´ ===================
        # å…ˆä¼°ç®— layer3 è¾“å‡ºå°ºå¯¸ï¼ˆè¾“å…¥ 2x128 â†’ ç»è¿‡ stride=2 ä¸‰æ¬¡ â†’ 128/(2^3)=16ï¼‰
        # æ‰€ä»¥ layer3 è¾“å‡º H=W=16ï¼ˆå› ä¸º 128 -> 64 -> 32 -> 16ï¼‰
        # self.reconstruction_head = nn.Sequential(
        # nn.Conv2d(1024, 256, kernel_size=1),  # 1x1 conv to reduce channels
        # nn.ReLU(inplace=True),
        # nn.Upsample(size=(8, 16), mode='bilinear', align_corners=False),  # ç›´æ¥ä¸Šé‡‡æ ·åˆ°ç›®æ ‡å°ºå¯¸
        # nn.Conv2d(256, 2, kernel_size=3, padding=1)  # è¾“å‡º 2 é€šé“
        # )
        # # =====================================================


    def _make_layer(self, in_channels, out_channels, blocks, stride):
        downsample = None
        # å½“æ­¥é•¿â‰ 1æˆ–è¾“å…¥é€šé“â‰ è¾“å‡ºé€šé“Ã—æ‰©å±•å› å­æ—¶ï¼Œéœ€è¦ä¸‹é‡‡æ ·
        if stride != 1 or in_channels != out_channels * Bottleneck.expansion:  # ä½¿ç”¨Bottleneck
            downsample = nn.Sequential(
                nn.Conv2d(in_channels, out_channels * Bottleneck.expansion, 
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels * Bottleneck.expansion)
            )
        layers = []
        # ç¬¬ä¸€ä¸ªå—ï¼šå¤„ç†é€šé“æ•°è½¬æ¢å’Œæ­¥é•¿
        layers.append(Bottleneck(in_channels, out_channels, stride, downsample))
        # æ›´æ–°å½“å‰é€šé“æ•°ï¼ˆè¾“å‡ºé€šé“Ã—æ‰©å±•å› å­ï¼‰
        self.in_channels = out_channels * Bottleneck.expansion
        # åç»­å—ï¼šè¾“å…¥è¾“å‡ºé€šé“æ•°ä¸€è‡´
        for _ in range(1, blocks):
            layers.append(Bottleneck(self.in_channels, out_channels))
        return nn.Sequential(*layers)

    def forward(self, x, is_feat=False, preact=False, return_reconstruction=False):
        # å¤„ç†å¤æ•°è¾“å…¥ï¼ˆé’ˆå¯¹Linkç­‰æ•°æ®é›†ï¼‰ï¼šå°†å®éƒ¨/è™šéƒ¨è½¬æ¢ä¸º2ä¸ªé€šé“
        if torch.is_complex(x):
            x = torch.view_as_real(x)
            # å°†æœ€åä¸€ä¸ªç»´åº¦ï¼ˆå®éƒ¨/è™šéƒ¨ 2ï¼‰ç§»åˆ° dim 1
            # ä¾‹å¦‚ (B, 1, L, 2) -> (B, 2, 1, L)
            dims = list(range(x.dim()))
            new_order = [0, len(dims)-1] + dims[1:-1]
            x = x.permute(*new_order).contiguous()

        # åŠ¨æ€é€‚é…è¾“å…¥ç»´åº¦è¿›è¡Œreshape
        batch_size = x.size(0)
        num_elements = x.view(batch_size, -1).size(1)
        
        if num_elements == 1024:
             # å¦‚æœè¾“å…¥å¤§å°ä¸º 1024ï¼Œå¯èƒ½æ˜¯å•é€šé“ 32x32ï¼Œæˆ–è€…æ˜¯ 2x16x32 ç­‰
             # å°è¯• reshape ä¸º [B, 1, 32, 32]
             x = x.view(batch_size, 1, 32, 32)
        elif num_elements == 2048:
             # å¦‚æœè¾“å…¥å¤§å°ä¸º 2048ï¼Œåˆ™ reshape ä¸º [B, 2, 32, 32]
             x = x.view(batch_size, 2, 32, 32)
        else:
             # å…¶ä»–æƒ…å†µå°è¯•æŒ‰ç…§ num_elements è‡ªåŠ¨æ¨å¯¼ï¼Œæˆ–è€…æŠ¥é”™
             # è¿™é‡Œå‡è®¾æˆ‘ä»¬å°½é‡é€‚é…æˆ Cx32x32 çš„å½¢å¼
             channels = num_elements // (32 * 32)
             if channels * 32 * 32 == num_elements:
                 x = x.view(batch_size, channels, 32, 32)
             else:
                  raise ValueError(f"Input size {num_elements} per sample cannot be reshaped to (C, 32, 32).")


        # print(f"è¾“å…¥å›¾åƒç»´åº¦: {x.shape}")  # è°ƒè¯•ï¼šæ‰“å°è¾“å…¥å›¾åƒç»´åº¦
        x1 = self.conv1(x)
        x2 = self.bn1(x1)
        x3 = self.relu(x2)
        x4 = self.maxpool(x3)

        x5 = self.layer1(x4)  # 256ç»´
        x6 = self.layer2(x5)  # 512ç»´
        x7 = self.layer3(x6)  # 1024ç»´

        x8 = self.avgpool(x7)
        x9 = torch.flatten(x8, 1)  # 1024ç»´
        x_intermediate = self.intermediate_fc(x9)  # 1000ç»´
        # x_intermediate_un = x_intermediate.unsqueeze(-1).unsqueeze(-1)  # è°ƒæ•´å½¢çŠ¶ä»¥åŒ¹é…å…¨è¿æ¥å±‚è¾“å…¥è¦æ±‚

        x10 = self.fc(x_intermediate)  # 21ç»´

        # if return_reconstruction:
        #     # é‡å»ºè·¯å¾„ï¼šä» layer3 è¾“å‡ºé‡å»º (2, 8, 16)
        #     rec_2d = self.reconstruction_head(x7)  # (B, 2, 8, 16)
        #     return rec_2d

        if is_feat:
            return [x3, x5, x6, x7], x10
        else:
            return x10
        

# import torch
# import torch.nn as nn

# class Bottleneck(nn.Module):
#     expansion = 4  # æ‰©å±•å› å­ï¼Œè¾“å‡ºé€šé“=out_channelsÃ—4
#     def __init__(self, in_channels, out_channels, stride=1, downsample=None):
#         super(Bottleneck, self).__init__()
#         self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)
#         self.bn1 = nn.BatchNorm2d(out_channels)
#         self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, 
#                                stride=stride, padding=1, bias=False)
#         self.bn2 = nn.BatchNorm2d(out_channels)
#         self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, 
#                                kernel_size=1, bias=False)
#         self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)
#         self.relu = nn.ReLU(inplace=True)
#         self.downsample = downsample

#     def forward(self, x):
#         identity = x
#         out = self.conv1(x)
#         out = self.bn1(out)
#         out = self.relu(out)
#         out = self.conv2(out)
#         out = self.bn2(out)
#         out = self.relu(out)
#         out = self.conv3(out)
#         out = self.bn3(out)
#         if self.downsample is not None:
#             identity = self.downsample(x)
#         out += identity
#         out = self.relu(out)
#         return out

# class ResNet20Real(nn.Module):
#     def __init__(self, num_classes=7):
#         super(ResNet20Real, self).__init__()
#         self.in_channels = 64  # æ”¹ä¸º64ï¼ŒåŒ¹é…æ•™å¸ˆæ¨¡å‹åˆå§‹é€šé“
#         self.conv1 = nn.Conv2d(2, 64, kernel_size=7, stride=2, padding=3, bias=False)  # æ¨¡ä»¿æ•™å¸ˆåˆå§‹å±‚
#         self.bn1 = nn.BatchNorm2d(64)
#         self.relu = nn.ReLU(inplace=True)
#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        
#         # ä¿®æ­£æ®‹å·®å—é€šé“æ•°é…ç½®ï¼š
#         # layer1ï¼šè¾“å…¥64é€šé“ â†’ è¾“å‡º256é€šé“ï¼ˆ64*4ï¼ŒåŒ¹é…Bottleneckæ‰©å±•å› å­ï¼‰
#         self.layer1 = self._make_layer(in_channels=64, out_channels=64, blocks=2, stride=1)
#         # layer2ï¼šè¾“å…¥256é€šé“ â†’ è¾“å‡º512é€šé“ï¼ˆ128*4ï¼‰
#         self.layer2 = self._make_layer(in_channels=256, out_channels=128, blocks=2, stride=2)
#         # layer3ï¼šè¾“å…¥512é€šé“ â†’ è¾“å‡º1024é€šé“ï¼ˆ256*4ï¼‰
#         self.layer3 = self._make_layer(in_channels=512, out_channels=256, blocks=2, stride=2)
        
#         self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # å·²æœ‰è‡ªé€‚åº”æ± åŒ–ï¼Œæ— éœ€ä¿®æ”¹
#         self.intermediate_fc = nn.Linear(1024, 1000)  # 1024â†’1000
#         self.fc = nn.Linear(1000, num_classes)  # æ³¨æ„ï¼šåŸä»£ç æ³¨é‡Šæ˜¯21ç»´ï¼Œnum_classesé»˜è®¤6ï¼Œéœ€ä¿æŒä¸€è‡´

#     def _make_layer(self, in_channels, out_channels, blocks, stride):
#         downsample = None
#         # å½“æ­¥é•¿â‰ 1æˆ–è¾“å…¥é€šé“â‰ è¾“å‡ºé€šé“Ã—æ‰©å±•å› å­æ—¶ï¼Œéœ€è¦ä¸‹é‡‡æ ·
#         if stride != 1 or in_channels != out_channels * Bottleneck.expansion:  # æ³¨æ„è¿™é‡Œä½¿ç”¨Bottleneckçš„expansion=4
#             downsample = nn.Sequential(
#                 nn.Conv2d(in_channels, out_channels * Bottleneck.expansion, 
#                           kernel_size=1, stride=stride, bias=False),
#                 nn.BatchNorm2d(out_channels * Bottleneck.expansion)
#             )
#         layers = []
#         # ç¬¬ä¸€ä¸ªå—ï¼šå¤„ç†é€šé“æ•°è½¬æ¢å’Œæ­¥é•¿
#         layers.append(Bottleneck(in_channels, out_channels, stride, downsample))
#         # æ›´æ–°å½“å‰é€šé“æ•°ï¼ˆè¾“å‡ºé€šé“Ã—æ‰©å±•å› å­ï¼‰
#         self.in_channels = out_channels * Bottleneck.expansion
#         # åç»­å—ï¼šè¾“å…¥è¾“å‡ºé€šé“æ•°ä¸€è‡´
#         for _ in range(1, blocks):
#             layers.append(Bottleneck(self.in_channels, out_channels))
#         return nn.Sequential(*layers)

#     def forward(self, x, is_feat=False, preact=False, return_reconstruction=False):
#         # å¤„ç†å¤æ•°è¾“å…¥ï¼ˆé’ˆå¯¹Linkç­‰æ•°æ®é›†ï¼‰ï¼šå°†å®éƒ¨/è™šéƒ¨è½¬æ¢ä¸º2ä¸ªé€šé“
#         if torch.is_complex(x):
#             x = torch.view_as_real(x)
#             # å°†æœ€åä¸€ä¸ªç»´åº¦ï¼ˆå®éƒ¨/è™šéƒ¨ 2ï¼‰ç§»åˆ° dim 1
#             # ä¾‹å¦‚ (B, 1, L, 2) -> (B, 2, 1, L)
#             dims = list(range(x.dim()))
#             new_order = [0, len(dims)-1] + dims[1:-1]
#             x = x.permute(*new_order).contiguous()

#         # ğŸ”§ æ ¸å¿ƒä¿®æ”¹1ï¼šæ›¿æ¢ç¡¬ç¼–ç çš„viewï¼ŒåŠ¨æ€é€‚é…è¾“å…¥å½¢çŠ¶ï¼ˆè§£å†³å‰ªæåå…ƒç´ æ•°ä¸åŒ¹é…é—®é¢˜ï¼‰
#         batch_size = x.size(0)
#         target_channels = 2
#         target_h, target_w = 32, 32
#         expected_total_elems = batch_size * target_channels * target_h * target_w
        
#         # æ ¡éªŒè¾“å…¥å…ƒç´ æ•°æ˜¯å¦åŒ¹é…ï¼Œé¿å…shapeé”™è¯¯
#         if x.numel() != expected_total_elems:
#             # å¯é€‰ï¼šè‡ªåŠ¨è°ƒæ•´ï¼ˆå¦‚æœè¾“å…¥ç»´åº¦å˜åŒ–ï¼‰ï¼Œæˆ–æŠ›å‡ºæ˜ç¡®é”™è¯¯
#             raise ValueError(
#                 f"è¾“å…¥å¼ é‡å…ƒç´ æ•°é”™è¯¯ï¼æœŸæœ› {expected_total_elems} (batch={batch_size}, 2Ã—32Ã—32), "
#                 f"å®é™… {x.numel()}ã€‚è¯·æ£€æŸ¥è¾“å…¥æ•°æ®å½¢çŠ¶ã€‚"
#             )
#         # é‡å¡‘è¾“å…¥ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼Œä½†å¢åŠ æ ¡éªŒï¼‰
#         x = x.view(batch_size, target_channels, target_h, target_w)

#         # ä»¥ä¸‹éƒ¨åˆ†æ— éœ€ä¿®æ”¹ï¼ˆå·²æœ‰è‡ªé€‚åº”æ± åŒ–ï¼Œå…¼å®¹é€šé“å‰ªæåçš„å½¢çŠ¶å˜åŒ–ï¼‰
#         x1 = self.conv1(x)
#         x2 = self.bn1(x1)
#         x3 = self.relu(x2)
#         x4 = self.maxpool(x3)

#         x5 = self.layer1(x4)  # 256ç»´ï¼ˆå‰ªæåé€šé“æ•°ä¼šå˜åŒ–ï¼Œè‡ªé€‚åº”æ± åŒ–å¯å…¼å®¹ï¼‰
#         x6 = self.layer2(x5)  # 512ç»´ï¼ˆå‰ªæåé€šé“æ•°å˜åŒ–ï¼‰
#         x7 = self.layer3(x6)  # 1024ç»´ï¼ˆå‰ªæåé€šé“æ•°å˜åŒ–ï¼‰

#         x8 = self.avgpool(x7)  # è‡ªé€‚åº”æ± åŒ–â†’(batch, C, 1, 1)ï¼ŒCä¸ºå‰ªæåçš„é€šé“æ•°
#         x9 = torch.flatten(x8, 1)  # å±•å¹³â†’(batch, C)ï¼Œå…¼å®¹ä»»æ„C
        
#         # ğŸ”§ æ ¸å¿ƒä¿®æ”¹2ï¼šåŠ¨æ€é€‚é…intermediate_fcçš„è¾“å…¥ç»´åº¦ï¼ˆå‰ªæåx9çš„ç»´åº¦â‰ 1024ï¼‰
#         # åŸintermediate_fcæ˜¯Linear(1024, 1000)ï¼Œå‰ªæåx9çš„ç»´åº¦å˜ä¸ºå‰ªæåçš„é€šé“æ•°ï¼Œéœ€é‡æ–°å®šä¹‰
#         # æ–¹æ¡ˆï¼šåˆå§‹åŒ–æ—¶ä¸å›ºå®šintermediate_fcï¼Œæˆ–åœ¨forwardä¸­åŠ¨æ€è°ƒæ•´
#         # æ›´ç®€å•çš„æ–¹æ¡ˆï¼šæ›¿æ¢å›ºå®šçš„intermediate_fcä¸ºåŠ¨æ€é€‚é…çš„çº¿æ€§å±‚
#         if hasattr(self, 'dynamic_intermediate_fc') and self.dynamic_intermediate_fc.in_features != x9.size(1):
#             # åŠ¨æ€é‡å»ºçº¿æ€§å±‚ï¼ŒåŒ¹é…å‰ªæåçš„è¾“å…¥ç»´åº¦
#             self.dynamic_intermediate_fc = nn.Linear(x9.size(1), 1000).to(x9.device)
#         elif not hasattr(self, 'dynamic_intermediate_fc'):
#             # é¦–æ¬¡è¿è¡Œï¼Œåˆå§‹åŒ–åŠ¨æ€çº¿æ€§å±‚ï¼ˆå…¼å®¹åŸå§‹1024ç»´ï¼‰
#             self.dynamic_intermediate_fc = nn.Linear(x9.size(1), 1000).to(x9.device)
#             # å¤åˆ¶åŸintermediate_fcçš„æƒé‡ï¼ˆå¦‚æœæ˜¯åŸå§‹ç»´åº¦ï¼‰
#             if x9.size(1) == 1024:
#                 self.dynamic_intermediate_fc.weight.data = self.intermediate_fc.weight.data
#                 self.dynamic_intermediate_fc.bias.data = self.intermediate_fc.bias.data
        
#         # ä½¿ç”¨åŠ¨æ€çº¿æ€§å±‚æ›¿ä»£åŸintermediate_fc
#         x_intermediate = self.dynamic_intermediate_fc(x9)  # (batch, 1000)
#         x10 = self.fc(x_intermediate)  # (batch, num_classes)

#         # ä¿ç•™åŸæœ‰è¿”å›é€»è¾‘
#         if is_feat:
#             return [x3, x5, x6, x7], x10
#         else:
#             return x10