# import torch
# import torch.nn as nn

# # class BasicBlock(nn.Module):
# #     expansion = 1
# #     def __init__(self, in_channels, out_channels, stride=1, downsample=None):
# #         super(BasicBlock, self).__init__()
# #         self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
# #         self.bn1 = nn.BatchNorm2d(out_channels)
# #         self.relu = nn.ReLU(inplace=True)
# #         self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)
# #         self.bn2 = nn.BatchNorm2d(out_channels)
# #         self.downsample = downsample

# #     def forward(self, x):
# #         identity = x
# #         out = self.conv1(x)
# #         out = self.bn1(out)
# #         out = self.relu(out)
# #         out = self.conv2(out)
# #         out = self.bn2(out)
# #         if self.downsample is not None:
# #             identity = self.downsample(x)
# #         out += identity
# #         out = self.relu(out)
# #         return out


#   # æ³¨æ„ï¼šéœ€è¦å°†BasicBlockæ›¿æ¢ä¸ºBottleneckå—ï¼ˆä¸æ•™å¸ˆæ¨¡å‹ä¸€è‡´ï¼‰
# class Bottleneck(nn.Module):
#     expansion = 4  # æ‰©å±•å› å­ï¼Œè¾“å‡ºé€šé“=out_channelsÃ—4
#     def __init__(self, in_channels, out_channels, stride=1, downsample=None):
#         super(Bottleneck, self).__init__()
#         self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)
#         self.bn1 = nn.BatchNorm2d(out_channels)
#         self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, 
#                                stride=stride, padding=1, bias=False)
#         self.bn2 = nn.BatchNorm2d(out_channels)
#         self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, 
#                                kernel_size=1, bias=False)
#         self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)
#         self.relu = nn.ReLU(inplace=True)
#         self.downsample = downsample

#     def forward(self, x):
#         identity = x
#         out = self.conv1(x)
#         out = self.bn1(out)
#         out = self.relu(out)
#         out = self.conv2(out)
#         out = self.bn2(out)
#         out = self.relu(out)
#         out = self.conv3(out)
#         out = self.bn3(out)
#         if self.downsample is not None:
#             identity = self.downsample(x)
#         out += identity
#         out = self.relu(out)
#         return out

# class ResNet20Real(nn.Module):
#     def __init__(self, num_classes=6):
#         super(ResNet20Real, self).__init__()
#         self.in_channels = 64  # æ”¹ä¸º64ï¼ŒåŒ¹é…æ•™å¸ˆæ¨¡å‹åˆå§‹é€šé“
#         self.conv1 = nn.Conv2d(2, 64, kernel_size=7, stride=2, padding=3, bias=False)  # æ¨¡ä»¿æ•™å¸ˆåˆå§‹å±‚
#         self.bn1 = nn.BatchNorm2d(64)
#         self.relu = nn.ReLU(inplace=True)
#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        
#         # ä¿®æ­£æ®‹å·®å—é€šé“æ•°é…ç½®ï¼š
#         # layer1ï¼šè¾“å…¥64é€šé“ â†’ è¾“å‡º256é€šé“ï¼ˆ64*4ï¼ŒåŒ¹é…Bottleneckæ‰©å±•å› å­ï¼‰
#         self.layer1 = self._make_layer(in_channels=64, out_channels=64, blocks=2, stride=1)
#         # layer2ï¼šè¾“å…¥256é€šé“ â†’ è¾“å‡º512é€šé“ï¼ˆ128*4ï¼‰
#         self.layer2 = self._make_layer(in_channels=256, out_channels=128, blocks=2, stride=2)
#         # layer3ï¼šè¾“å…¥512é€šé“ â†’ è¾“å‡º1024é€šé“ï¼ˆ256*4ï¼‰
#         self.layer3 = self._make_layer(in_channels=512, out_channels=256, blocks=2, stride=2)
        
#         self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
#         self.intermediate_fc = nn.Linear(1024, 1000)  # 1024â†’1000
#         self.fc = nn.Linear(1000, num_classes)  # 1000â†’21

#         # =================== æ–°å¢ï¼šé¢‘åŸŸé‡å»ºå¤´ ===================
#         # å…ˆä¼°ç®— layer3 è¾“å‡ºå°ºå¯¸ï¼ˆè¾“å…¥ 2x128 â†’ ç»è¿‡ stride=2 ä¸‰æ¬¡ â†’ 128/(2^3)=16ï¼‰
#         # æ‰€ä»¥ layer3 è¾“å‡º H=W=16ï¼ˆå› ä¸º 128 -> 64 -> 32 -> 16ï¼‰
#         # self.reconstruction_head = nn.Sequential(
#         # nn.Conv2d(1024, 256, kernel_size=1),  # 1x1 conv to reduce channels
#         # nn.ReLU(inplace=True),
#         # nn.Upsample(size=(8, 16), mode='bilinear', align_corners=False),  # ç›´æ¥ä¸Šé‡‡æ ·åˆ°ç›®æ ‡å°ºå¯¸
#         # nn.Conv2d(256, 2, kernel_size=3, padding=1)  # è¾“å‡º 2 é€šé“
#         # )
#         # # =====================================================


#     def _make_layer(self, in_channels, out_channels, blocks, stride):
#         downsample = None
#         # å½“æ­¥é•¿â‰ 1æˆ–è¾“å…¥é€šé“â‰ è¾“å‡ºé€šé“Ã—æ‰©å±•å› å­æ—¶ï¼Œéœ€è¦ä¸‹é‡‡æ ·
#         if stride != 1 or in_channels != out_channels * Bottleneck.expansion:  # æ³¨æ„è¿™é‡Œä½¿ç”¨Bottleneckçš„expansion=4
#             downsample = nn.Sequential(
#                 nn.Conv2d(in_channels, out_channels * Bottleneck.expansion, 
#                           kernel_size=1, stride=stride, bias=False),
#                 nn.BatchNorm2d(out_channels * Bottleneck.expansion)
#             )
#         layers = []
#         # ç¬¬ä¸€ä¸ªå—ï¼šå¤„ç†é€šé“æ•°è½¬æ¢å’Œæ­¥é•¿
#         layers.append(Bottleneck(in_channels, out_channels, stride, downsample))
#         # æ›´æ–°å½“å‰é€šé“æ•°ï¼ˆè¾“å‡ºé€šé“Ã—æ‰©å±•å› å­ï¼‰
#         self.in_channels = out_channels * Bottleneck.expansion
#         # åç»­å—ï¼šè¾“å…¥è¾“å‡ºé€šé“æ•°ä¸€è‡´
#         for _ in range(1, blocks):
#             layers.append(Bottleneck(self.in_channels, out_channels))
#         return nn.Sequential(*layers)

#     def forward(self, x, is_feat=False, preact=False, return_reconstruction=False):
#         # Adjust input shape to [batch_size, channels, height, width]
#         x = x.view(x.size(0), 2, 24, 25)  # å°†å®½åº¦ 4096 è½¬æ¢ä¸º 64x64 çš„é«˜åº¦å’Œå®½åº¦

#         # print(f"è¾“å…¥å›¾åƒç»´åº¦: {x.shape}")  # è°ƒè¯•ï¼šæ‰“å°è¾“å…¥å›¾åƒç»´åº¦
#         x1 = self.conv1(x)
#         x2 = self.bn1(x1)
#         x3 = self.relu(x2)
#         x4 = self.maxpool(x3)

#         x5 = self.layer1(x4)  # 256ç»´
#         x6 = self.layer2(x5)  # 512ç»´
#         x7 = self.layer3(x6)  # 1024ç»´

#         x8 = self.avgpool(x7)
#         x9 = torch.flatten(x8, 1)  # 1024ç»´
#         x_intermediate = self.intermediate_fc(x9)  # 1000ç»´
#         # x_intermediate_un = x_intermediate.unsqueeze(-1).unsqueeze(-1)  # è°ƒæ•´å½¢çŠ¶ä»¥åŒ¹é…å…¨è¿æ¥å±‚è¾“å…¥è¦æ±‚

#         x10 = self.fc(x_intermediate)  # 21ç»´

#         # if return_reconstruction:
#         #     # é‡å»ºè·¯å¾„ï¼šä» layer3 è¾“å‡ºé‡å»º (2, 8, 16)
#         #     rec_2d = self.reconstruction_head(x7)  # (B, 2, 8, 16)
#         #     return rec_2d

#         if is_feat:
#             return [x3, x5, x6, x7], x10
#         else:
#             return x10


import torch
import torch.nn as nn

class Bottleneck(nn.Module):
    expansion = 4  # æ‰©å±•å› å­ï¼Œè¾“å‡ºé€šé“=out_channelsÃ—4
    def __init__(self, in_channels, out_channels, stride=1, downsample=None):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, 
                               stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, 
                               kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample

    def forward(self, x):
        identity = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)
        out = self.conv3(out)
        out = self.bn3(out)
        if self.downsample is not None:
            identity = self.downsample(x)
        out += identity
        out = self.relu(out)
        return out

class ResNet20Real(nn.Module):
    def __init__(self, num_classes=6):
        super(ResNet20Real, self).__init__()
        self.in_channels = 64  # æ”¹ä¸º64ï¼ŒåŒ¹é…æ•™å¸ˆæ¨¡å‹åˆå§‹é€šé“
        self.conv1 = nn.Conv2d(2, 64, kernel_size=7, stride=2, padding=3, bias=False)  # æ¨¡ä»¿æ•™å¸ˆåˆå§‹å±‚
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        
        # ä¿®æ­£æ®‹å·®å—é€šé“æ•°é…ç½®ï¼š
        # layer1ï¼šè¾“å…¥64é€šé“ â†’ è¾“å‡º256é€šé“ï¼ˆ64*4ï¼ŒåŒ¹é…Bottleneckæ‰©å±•å› å­ï¼‰
        self.layer1 = self._make_layer(in_channels=64, out_channels=64, blocks=2, stride=1)
        # layer2ï¼šè¾“å…¥256é€šé“ â†’ è¾“å‡º512é€šé“ï¼ˆ128*4ï¼‰
        self.layer2 = self._make_layer(in_channels=256, out_channels=128, blocks=2, stride=2)
        # layer3ï¼šè¾“å…¥512é€šé“ â†’ è¾“å‡º1024é€šé“ï¼ˆ256*4ï¼‰
        self.layer3 = self._make_layer(in_channels=512, out_channels=256, blocks=2, stride=2)
        
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # å·²æœ‰è‡ªé€‚åº”æ± åŒ–ï¼Œæ— éœ€ä¿®æ”¹
        self.intermediate_fc = nn.Linear(1024, 1000)  # 1024â†’1000
        self.fc = nn.Linear(1000, num_classes)  # æ³¨æ„ï¼šåŸä»£ç æ³¨é‡Šæ˜¯21ç»´ï¼Œnum_classesé»˜è®¤6ï¼Œéœ€ä¿æŒä¸€è‡´

    def _make_layer(self, in_channels, out_channels, blocks, stride):
        downsample = None
        # å½“æ­¥é•¿â‰ 1æˆ–è¾“å…¥é€šé“â‰ è¾“å‡ºé€šé“Ã—æ‰©å±•å› å­æ—¶ï¼Œéœ€è¦ä¸‹é‡‡æ ·
        if stride != 1 or in_channels != out_channels * Bottleneck.expansion:  # æ³¨æ„è¿™é‡Œä½¿ç”¨Bottleneckçš„expansion=4
            downsample = nn.Sequential(
                nn.Conv2d(in_channels, out_channels * Bottleneck.expansion, 
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels * Bottleneck.expansion)
            )
        layers = []
        # ç¬¬ä¸€ä¸ªå—ï¼šå¤„ç†é€šé“æ•°è½¬æ¢å’Œæ­¥é•¿
        layers.append(Bottleneck(in_channels, out_channels, stride, downsample))
        # æ›´æ–°å½“å‰é€šé“æ•°ï¼ˆè¾“å‡ºé€šé“Ã—æ‰©å±•å› å­ï¼‰
        self.in_channels = out_channels * Bottleneck.expansion
        # åç»­å—ï¼šè¾“å…¥è¾“å‡ºé€šé“æ•°ä¸€è‡´
        for _ in range(1, blocks):
            layers.append(Bottleneck(self.in_channels, out_channels))
        return nn.Sequential(*layers)

    def forward(self, x, is_feat=False, preact=False, return_reconstruction=False):
        # Input x shape: (batch_size, 600) complex tensor
        # Extract real and imaginary parts, reshape to (batch_size, 2, 20, 30)
        batch_size = x.shape[0]
        
        # Extract real and imaginary parts
        x_real = torch.real(x)  # (batch_size, 600)
        x_imag = torch.imag(x)  # (batch_size, 600)
        
        # Reshape each to (batch_size, 1, 20, 30)
        x_real = x_real.reshape(batch_size, 1, 20, 30)
        x_imag = x_imag.reshape(batch_size, 1, 20, 30)
        
        # Concatenate to (batch_size, 2, 20, 30)
        x = torch.cat([x_real, x_imag], dim=1)

        # ä»¥ä¸‹éƒ¨åˆ†æ— éœ€ä¿®æ”¹ï¼ˆå·²æœ‰è‡ªé€‚åº”æ± åŒ–ï¼Œå…¼å®¹é€šé“å‰ªæåçš„å½¢çŠ¶å˜åŒ–ï¼‰
        x1 = self.conv1(x)
        x2 = self.bn1(x1)
        x3 = self.relu(x2)
        x4 = self.maxpool(x3)

        x5 = self.layer1(x4)  # 256ç»´ï¼ˆå‰ªæåé€šé“æ•°ä¼šå˜åŒ–ï¼Œè‡ªé€‚åº”æ± åŒ–å¯å…¼å®¹ï¼‰
        x6 = self.layer2(x5)  # 512ç»´ï¼ˆå‰ªæåé€šé“æ•°å˜åŒ–ï¼‰
        x7 = self.layer3(x6)  # 1024ç»´ï¼ˆå‰ªæåé€šé“æ•°å˜åŒ–ï¼‰

        x8 = self.avgpool(x7)  # è‡ªé€‚åº”æ± åŒ–â†’(batch, C, 1, 1)ï¼ŒCä¸ºå‰ªæåçš„é€šé“æ•°
        x9 = torch.flatten(x8, 1)  # å±•å¹³â†’(batch, C)ï¼Œå…¼å®¹ä»»æ„C
        
        # ä½¿ç”¨å›ºå®šçš„intermediate_fcï¼ˆæ­£å¸¸è®­ç»ƒä½¿ç”¨ï¼‰
        x_intermediate = self.intermediate_fc(x9)  # (batch, 1000)
        x10 = self.fc(x_intermediate)  # (batch, num_classes)
        
        # ğŸ”§ æ ¸å¿ƒä¿®æ”¹2ï¼šåŠ¨æ€é€‚é…intermediate_fcçš„è¾“å…¥ç»´åº¦ï¼ˆå‰ªæåx9çš„ç»´åº¦â‰ 1024ï¼‰
        # åŸintermediate_fcæ˜¯Linear(1024, 1000)ï¼Œå‰ªæåx9çš„ç»´åº¦å˜ä¸ºå‰ªæåçš„é€šé“æ•°ï¼Œéœ€é‡æ–°å®šä¹‰
        # æ–¹æ¡ˆï¼šåˆå§‹åŒ–æ—¶ä¸å›ºå®šintermediate_fcï¼Œæˆ–åœ¨forwardä¸­åŠ¨æ€è°ƒæ•´
        # æ›´ç®€å•çš„æ–¹æ¡ˆï¼šæ›¿æ¢å›ºå®šçš„intermediate_fcä¸ºåŠ¨æ€é€‚é…çš„çº¿æ€§å±‚
        # if hasattr(self, 'dynamic_intermediate_fc') and self.dynamic_intermediate_fc.in_features != x9.size(1):
        #     # åŠ¨æ€é‡å»ºçº¿æ€§å±‚ï¼ŒåŒ¹é…å‰ªæåçš„è¾“å…¥ç»´åº¦
        #     self.dynamic_intermediate_fc = nn.Linear(x9.size(1), 1000).to(x9.device)
        # elif not hasattr(self, 'dynamic_intermediate_fc'):
        #     # é¦–æ¬¡è¿è¡Œï¼Œåˆå§‹åŒ–åŠ¨æ€çº¿æ€§å±‚ï¼ˆå…¼å®¹åŸå§‹1024ç»´ï¼‰
        #     self.dynamic_intermediate_fc = nn.Linear(x9.size(1), 1000).to(x9.device)
        #     # å¤åˆ¶åŸintermediate_fcçš„æƒé‡ï¼ˆå¦‚æœæ˜¯åŸå§‹ç»´åº¦ï¼‰
        #     if x9.size(1) == 1024:
        #         self.dynamic_intermediate_fc.weight.data = self.intermediate_fc.weight.data
        #         self.dynamic_intermediate_fc.bias.data = self.intermediate_fc.bias.data
        # 
        # # ä½¿ç”¨åŠ¨æ€çº¿æ€§å±‚æ›¿ä»£åŸintermediate_fc
        # x_intermediate = self.dynamic_intermediate_fc(x9)  # (batch, 1000)
        # x10 = self.fc(x_intermediate)  # (batch, num_classes)

        # ä¿ç•™åŸæœ‰è¿”å›é€»è¾‘
        if is_feat:
            return [x3, x5, x6, x7], x10
        else:
            return x10